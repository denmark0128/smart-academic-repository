# Generated by Django 5.2.4 on 2025-11-01 10:13

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('staff', '0002_searchsettings_tag_cache_timeout_and_more'),
    ]

    operations = [
        migrations.CreateModel(
            name='LlamaSettings',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('repo_id', models.CharField(default='unsloth/gemma-3-1b-it-GGUF', max_length=255)),
                ('model_filename', models.CharField(default='gemma-3-1b-it-BF16.gguf', max_length=255)),
                ('model_seed', models.IntegerField(default=4)),
                ('n_ctx', models.IntegerField(default=32000, help_text='Context window size')),
                ('system_prompt', models.TextField(default='You are an academic summarizer that writes clear and concise summaries.')),
                ('user_prompt_template', models.TextField(default='Summarize this paper concisely and detailed:\n\n{text}')),
                ('generation_seed', models.IntegerField(default=2)),
                ('temperature', models.FloatField(blank=True, default=0.7, help_text='Controls randomness. Lower is more deterministic. (e.g., 0.7)', null=True)),
                ('max_tokens', models.IntegerField(blank=True, help_text='Max tokens to generate. Null means no limit.', null=True)),
                ('top_p', models.FloatField(blank=True, default=0.95, help_text='Nucleus sampling. (e.g., 0.95)', null=True)),
                ('settings_cache_timeout', models.IntegerField(default=900, help_text='How long to cache these settings (seconds). 15 min = 900.')),
            ],
            options={
                'verbose_name_plural': 'Llama Settings',
            },
        ),
    ]
